Vector embeddings are the foundation of semantic search in AI applications. By storing embeddings as document properties in Azure Cosmos DB for NoSQL, you create a unified data model that keeps vectors alongside metadata in the same database. This approach simplifies architecture by eliminating the need for separate vector databases while enabling efficient storage and retrieval of high-dimensional data.

## Understand vector embeddings

Embeddings are numerical representations of data generated by machine learning models. Services like Azure OpenAI, Hugging Face, and other providers offer embedding APIs that convert text, images, or other content into vectors. These arrays of floating-point numbers capture semantic meaning. When content is similar in meaning, the resulting vectors are close together in the high-dimensional vector space.

Different embedding models produce vectors with different dimensions. Azure OpenAI's `text-embedding-ada-002` model generates 1,536-dimensional vectors, while `text-embedding-3-large` produces 3,072 dimensions. The model choice affects both storage requirements and semantic quality. Larger dimensions often capture more nuanced meaning but require more storage space.

For the customer support knowledge base scenario, each document's text content is converted to an embedding that captures its semantic meaning. Two documents about WiFi troubleshooting produce similar vectors even if one uses "wireless connection" and the other uses "WiFi network." This similarity enables semantic search that understands meaning rather than matching exact keywords.

## Design document structure for vector storage

When designing documents for vector search, store embeddings as array properties alongside your business data. A well-designed document structure includes the original content, relevant metadata for filtering, and the embedding vector itself. This approach keeps all related data together and supports both semantic queries and traditional filters.

Consider a knowledge base document for the support system. The document includes identification fields, the content that users search, metadata for filtering results, and the embedding generated from the content:

```json
{
    "id": "doc-12345",
    "title": "Troubleshooting wireless network connections",
    "content": "This guide covers common WiFi connectivity issues including connection drops, slow speeds, and authentication failures...",
    "category": "networking",
    "productId": "router-x100",
    "createdDate": "2024-06-15T10:30:00Z",
    "embedding": [0.023, -0.041, 0.067, ...]
}
```

The `embedding` property contains 1,536 floating-point numbers generated by the embedding model. When users search for "internet not working," the query text is converted to an embedding, and documents with similar embeddings are returned—including this document even though it doesn't contain the exact phrase "internet not working."

## Configure container vector policies

Azure Cosmos DB requires a vector policy when you create a container that stores embeddings. The vector policy specifies essential information about your embedding properties that the database engine uses for efficient similarity search. You must define the policy at container creation time because vector policies can't be modified after the container exists.

> [!NOTE]
> Before using vector search, you must enable the feature on your Azure Cosmos DB account. You can enable it through the Azure portal under **Settings > Features > Vector Search for NoSQL API**, or by using the Azure CLI: `az cosmosdb update --capabilities EnableNoSQLVectorSearch`. The feature is auto-approved but might take 15 minutes to take effect.

A vector policy contains the following configuration elements for each embedding path:

- **path:** The JSON path to the embedding property within documents (required)
- **dataType:** The data type of vector elements. Supported values are `float32`, `float16`, `int8`, or `uint8` (default: `float32`)
- **dimensions:** The number of elements in each vector, which must match your embedding model (default: 1,536)
- **distanceFunction:** The metric for calculating similarity. Supported values are `cosine`, `dotproduct`, or `euclidean` (default: `cosine`)

The following example shows a vector policy for a container storing embeddings at the `/embedding` path:

```python
vector_embedding_policy = {
    "vectorEmbeddings": [
        {
            "path": "/embedding",
            "dataType": "float32",
            "distanceFunction": "cosine",
            "dimensions": 1536
        }
    ]
}
```

## Choose the appropriate distance function

The distance function determines how the database calculates similarity between vectors. Your choice affects both the interpretation of similarity scores and query performance. Azure Cosmos DB supports three distance functions, each suited to different use cases.

**Cosine similarity** measures the angle between two vectors, making it ideal for comparing text embeddings where magnitude shouldn't affect similarity. Azure OpenAI embeddings are normalized, meaning cosine similarity works well for them. The `VectorDistance` function returns cosine similarity scores ranging from -1 (least similar) to +1 (most similar), with most practical results falling between 0 and 1. Higher scores indicate greater similarity between the query and document vectors.

**Dot product** measures both angle and magnitude. For normalized vectors like those from Azure OpenAI, dot product results are mathematically identical to cosine similarity but can be slightly faster to compute. Use dot product when your embeddings are guaranteed to be normalized and you want maximum query performance.

**Euclidean distance** measures the straight-line distance between two points in the vector space. Scores range from 0 (identical) to positive infinity (most different). This function suits specialized use cases where both direction and magnitude matter, but it's less common for text embeddings.

For the support knowledge base using Azure OpenAI embeddings, cosine similarity is the recommended choice because it handles the normalized embeddings correctly and provides intuitive similarity scoring.

## Select data types for storage efficiency

Vector data types offer a trade-off between precision and storage cost. The `float32` type provides full precision but consumes the most storage. Using `float16` reduces storage by 50 percent with minimal impact on search quality—for most AI applications, this precision reduction is imperceptible in search results.

The `int8` and `uint8` types provide further compression through quantization. These types work with embeddings that have been quantized (converted from floating-point to integer values) and offer significant storage savings for large-scale deployments. However, quantization typically happens as part of a larger optimization strategy and requires careful evaluation of accuracy trade-offs.

For most AI applications starting with vector search, `float32` provides the best balance of accuracy and simplicity. Consider `float16` when storage costs become significant and your testing shows acceptable search quality.

## Store multiple vector types per document

Some applications need multiple embeddings per document to enable different search strategies. For example, you might generate separate embeddings for a document's title and its full content, or store both text and image embeddings. Azure Cosmos DB supports multiple vector paths in a single container by defining multiple entries in the vector policy.

The following vector policy configures two embedding paths with different dimensions and distance functions:

```python
vector_embedding_policy = {
    "vectorEmbeddings": [
        {
            "path": "/titleEmbedding",
            "dataType": "float32",
            "distanceFunction": "cosine",
            "dimensions": 1536
        },
        {
            "path": "/contentEmbedding",
            "dataType": "float32",
            "distanceFunction": "cosine",
            "dimensions": 1536
        }
    ]
}
```

With this configuration, queries can search by title similarity, content similarity, or combine both using hybrid search techniques covered later in this module.

## Configure vector indexes in the indexing policy

Vector indexes accelerate similarity searches by organizing vectors for efficient querying. Without an index, every query performs a full scan comparing the query vector against all stored vectors—a process that becomes slow and expensive as your dataset grows.

Azure Cosmos DB offers three vector index types:

- **flat:** Stores vectors in the main index and performs exact brute-force search. Provides 100 percent recall but limits vectors to 505 dimensions. Best for small datasets or when exact results are required.
- **quantizedFlat:** Compresses vectors before indexing for improved efficiency. Still performs brute-force search but with reduced latency and RU cost. Supports up to 4,096 dimensions and is recommended for datasets up to approximately 50,000 vectors per physical partition.
- **diskANN:** Uses Microsoft Research's DiskANN algorithm for fast approximate search. Supports up to 4,096 dimensions and provides the best performance for large datasets with millions of vectors while maintaining high accuracy.

The following indexing policy creates a DiskANN index on the embedding path and excludes the embedding from standard range indexes:

```python
indexing_policy = {
    "indexingMode": "consistent",
    "automatic": True,
    "includedPaths": [
        {"path": "/*"}
    ],
    "excludedPaths": [
        {"path": "/\"_etag\"/?"},
        {"path": "/embedding/*"}
    ],
    "vectorIndexes": [
        {"path": "/embedding", "type": "diskANN"}
    ]
}
```

Excluding the embedding path from `includedPaths` is important because embedding arrays don't benefit from standard range indexes. Including them wastes storage and increases write costs without providing value for queries.

> [!NOTE]
> The `quantizedFlat` and `diskANN` index types require at least 1,000 vectors before the index becomes effective. With fewer vectors, queries fall back to full scans, which might result in higher RU charges.

## Create a container with vector policies

When creating a container for vector search, you specify both the vector embedding policy and the indexing policy. These policies must be set at creation time. Vector policies can't be modified on existing containers, and vector search is only supported on new containers.

The following code creates a container configured for vector search in the customer support knowledge base:

```python
from azure.cosmos import CosmosClient, PartitionKey

# Vector embedding policy
vector_embedding_policy = {
    "vectorEmbeddings": [
        {
            "path": "/embedding",
            "dataType": "float32",
            "distanceFunction": "cosine",
            "dimensions": 1536
        }
    ]
}

# Indexing policy with vector index
indexing_policy = {
    "indexingMode": "consistent",
    "automatic": True,
    "includedPaths": [
        {"path": "/*"}
    ],
    "excludedPaths": [
        {"path": "/\"_etag\"/?"},
        {"path": "/embedding/*"}
    ],
    "vectorIndexes": [
        {"path": "/embedding", "type": "diskANN"}
    ]
}

# Create container with vector support
container = database.create_container(
    id="knowledge-base",
    partition_key=PartitionKey(path="/category"),
    indexing_policy=indexing_policy,
    vector_embedding_policy=vector_embedding_policy
)
```

The partition key choice (`/category`) aligns with expected query patterns. Support agents typically search within specific product categories, allowing queries to target a single partition for better performance.

## Insert documents with embeddings

After creating the container, you can insert documents with their embeddings. The workflow involves generating an embedding from the document content using your embedding API, then inserting the complete document including the embedding.

The following example demonstrates generating an embedding with Azure OpenAI and inserting the document:

```python
from openai import AzureOpenAI
from azure.cosmos import CosmosClient

# Initialize clients
openai_client = AzureOpenAI(
    api_key=api_key,
    api_version="2024-02-01",
    azure_endpoint=endpoint
)

cosmos_client = CosmosClient(cosmos_endpoint, credential=cosmos_key)
container = cosmos_client.get_database_client("support-db").get_container_client("knowledge-base")

# Generate embedding from document content
document_text = f"{title} {content}"
response = openai_client.embeddings.create(
    input=document_text,
    model="text-embedding-ada-002"
)
embedding = response.data[0].embedding

# Create document with embedding
document = {
    "id": document_id,
    "title": title,
    "content": content,
    "category": category,
    "productId": product_id,
    "createdDate": created_date,
    "embedding": embedding
}

# Insert or update document
container.upsert_item(document)
```

Using `upsert_item` handles both new documents and updates to existing documents. When the document content changes, regenerate the embedding and upsert the updated document to keep the embedding synchronized with the current content.

## Additional resources

- [Vector search in Azure Cosmos DB for NoSQL](/azure/cosmos-db/nosql/vector-search)
- [Index and query vectors in Python](/azure/cosmos-db/how-to-python-vector-index-query)
- [Azure OpenAI embeddings](/azure/ai-services/openai/concepts/understand-embeddings)

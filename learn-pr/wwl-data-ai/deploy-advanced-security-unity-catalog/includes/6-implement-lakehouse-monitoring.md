In this unit, we will use a heart rate measurements table as our guiding example to learn about lakehouse monitoring. You'll explore how to set up a monitoring system, understand the types of metrics it can capture, and learn how to interpret those insights. Along the way, we'll highlight best practices for identifying issues such as missing values or unusual readings, ensuring your data remains accurate and reliable.

## Scenario: Heart Rate Measurements Table

Imagine we have a Delta table called **`heart_rate_measurements`** that stores readings from wearable devices. Each row contains:

- `device_id` (Integer) – an ID for the device that took the measurement
- `mrm` (Text) – a code (for example, medical record number or reading ID)
- `name` (Text) – the person's name (or could be blank if not provided)
- `time` (Timestamp) – when the measurement was taken
- `heartrate` (Decimal) – the heart rate value recorded

For example, a few rows might look like:


| device_id |   mrm    |      name       |           time            | heartrate |
|-----------|----------|-----------------|---------------------------|-----------|
|    23     | 40580129 | Nicholas Spears | 2025-10-01T00:01:58+00:00 |   54.0122 |
|    17     | 52804177 | Lynn Russell    | 2025-10-01T00:02:55+00:00 |   92.5136 |
|    37     | 65300842 | Samuel Hughes   | 2025-10-01T00:08:58+00:00 |   52.1354 |

As data engineers, we **want to ensure the quality of this data over time**. Important questions include: Are there any **missing names** (nulls)? Do we have heart rate readings that are **zero** (which might indicate sensor errors or placeholders)? What are the typical ranges of heart rates, and did we ever get an **outlier** reading that looks unreasonable? Manually checking these can be labor-intensive; instead, we can leverage Databricks Lakehouse Monitoring to automate these checks and provide a dashboard of data quality metrics.

## Setting Up a Data Monitor (Snapshot Profile)

To monitor a Delta table like `heart_rate_measurements`, you need to **create a monitor** and attach it to that table. Databricks makes this easy through the Unity Catalog Data Explorer UI: 

<screenshot>

1. Open the Table in Data Explorer: Navigate to the table (for example, via the Catalog explorer). Go to the **Quality** tab for the table. If no monitor exists yet, you'll see an option to **"Get Started"** with Lakehouse Monitoring.

2. **Choose a Profile Type**: When creating a monitor, you select a profile type that determines how data is analyzed. There are three types: Snapshot, Time series and Inference.  For our scenario, we select **Snapshot** since we want to profile the entire `heart_rate_measurements` table state each time. 

3. **Configure Refresh Schedule**: Decide how the monitor should run:
    - You can have it **refresh on a schedule** (for example, every hour or every day) to continuously monitor new data, or
    - Set it to **refresh manually** on-demand. In a development or demo setting, manual refresh is fine (as in our original lab scenario). In production, you might schedule it to run after each ETL job or daily at a certain time.

4. **Create the Monitor**: After choosing the settings, select **Create**. Databricks will set up the necessary monitoring infrastructure. Under the hood, this creates two Delta tables (a p**rofile metrics** table and a **drift metrics** table in a specified monitoring schema) and a default dashboard for your monitor. The monitor will then perform its first data scan. (Note: The initial profiling may take a few minutes depending on table size, since it's computing a comprehensive set of statistics.)

**Monitor via API**: Alternatively, monitors can be created via the Databricks API/SDK. For example, you could use Python with the Databricks SDK to create monitors programmatically for many tables. In our context, we'll stick to the UI for clarity, but advanced workflows might automate monitor creation for all important tables in a schema.

**Unity Catalog Requirements**: Keep in mind that Lakehouse Monitoring works for tables in Unity Catalog (Delta tables). Ensure Unity Catalog is enabled and you have adequate permissions (like MANAGE SCHEMA on the schema containing the table) to create a monitor. 

## What Metrics Does the Monitor Capture?

Once the monitor is created and run, it computes a rich set of **profile metrics** that describe the data in `heart_rate_measurements`. These metrics are stored in the profile metrics table (accessible via Unity Catalog) and also visualized in the autogenerated dashboard. Here are some key types of metrics Lakehouse Monitoring provides out-of-the-box:

- **Record Count & Volume**: Total number of rows in the table, and potentially trends of row counts over time. For example, if yesterday there were 1,000 readings and today 1,200, that trend is captured. Sudden drops or spikes in row count can indicate pipeline issues (missing data or duplicates).

- **Missing Data Indicators**: The count and percentage of null values in each column. If our name column has some missing entries, the monitor would report something like "Name – 5 nulls (5%)" for that run. This lets us quickly spot if missing data is creeping in. Similarly, although zero is a valid numeric value, we might treat 0 in `heartrate` as a special case (for instance, a malfunctioning device might report 0). Lakehouse Monitoring tracks distribution of values, so an influx of zeros would show up in metrics like min value or in a distribution histogram, and we could also add a custom metric to specifically count zeros if needed.

- **Statistical Distribution (Numeric columns)**: Summary stats like min, max, mean, standard deviation, and percentiles (for example, 25th, 50th, 75th, 90th percentile) for numeric columns. For `heartrate`, the monitor will compute the minimum and maximum heart rate observed, which immediately flags outliers:
  
  - Example: If normally heart rates range from 50 to 120, but one record has `heartrate = 1000052.13`, the max value metric will jump to ~1,000,052. This clearly signals an anomaly. Similarly, the mean might be skewed, and the distribution chart would show an outlier point.
  
  - The 90th percentile heart rate might be, say, 100 in normal operation; if it suddenly becomes much higher, that indicates a shift in the data distribution.

- **Categorical Distribution (Text columns)**: For string columns (like `mrm` or `name)`, the monitor can report distinct counts or top frequent values. For example, it might track how many distinct devices (`device_id`) are contributing data, or if a categorical column's value frequencies change significantly (which could indicate a new category appearing or an old one dropping off unexpectedly). This helps in detecting categorical anomalies. In our heart rate scenario, `name` might not be a strict category to monitor distribution of, but if we had a column like `device_type` or `sensor_location`, changes there would be notable.

- **Data Freshness**: Although not a "profile" metric per se, monitors also note when the table was last updated and can flag if data hasn't arrived as expected (staleness). If our heart rate sensors are supposed to send data every hour and nothing came in a day, the monitor could mark the table as stale (freshness check failed).

- **Drift Metrics**: If you have a baseline dataset or simply as days progress, the monitor calculates drift statistics – essentially how today's metrics differ from yesterday's or from a reference dataset. For example, it might compute that the average heart rate today is +5 different from the average a week ago, or that the percentage of null names increased by 2% compared to the baseline. These drift metrics are stored in a separate drift metrics table. They highlight changes in data quality over time (for example, "null percentage increased by +5%" or "row count is 50% lower than expected").

All these metrics are recorded in Delta tables, meaning you can query them with SQL for custom analysis or join them with other operational data. Databricks also provides a default dashboard to visualize these metrics so you don't have to build charts from scratch.

To illustrate, here are some example metrics after an initial monitor run on our table (imagine the table initially had 50 records with no nulls or zeros):

xxx

## Visualizing Data Quality with the Dashboard

After the monitor runs, you can view the automatically generated dashboard for the monitor. This dashboard provides a visual overview of the metrics we discussed, making it easier to spot trends and outliers at a glance. To access it, you would typically select "**View dashboard**" from the table's Quality tab, or find it in the Databricks SQL workspace (it's saved under your user's dashboards).

The dashboard is interactive and has filters (widgets) at the top. For a Snapshot profile, you might see a time range selector (to choose the date range of monitor results to display, for example, last 7 days, or custom start/end), as well as possible filters for specific slices or groupings if those were configured. In our example, because snapshot profiles recompute on the whole table, we might simply filter by date of monitor run (each run produces one set of metrics).

On the left side or top, you'll see a list of metrics and charts available. For instance, one chart might show **Row Count Over Time**, another might show % Nulls per Column, another might display a distribution histogram for heartrate values. You can select these to navigate the dashboard.

Each visualization is backed by a query on the metric tables. By default, the Snapshot dashboard might include:
  - A table or chart of **Summary Statistics** for each column (for example, count, mean, min, max, % null, distinct count).
  - A chart for **Numeric Distributions** (perhaps a histogram or boxplot of the heart rate values).
  - If there were any time-based grouping or slices, charts for those as well.

Since our monitor is snapshot (no time window grouping by default), the dashboard might show the latest profile metrics and possibly comparisons to the previous profile if you adjust filters.

You must attach a SQL warehouse (compute engine) to the dashboard to execute the queries when you view it. In practice, you'd ensure a SQL endpoint is selected (for example, a shared warehouse) when opening it so that all charts can load the latest data.

The dashboard is fully customizable. You can edit existing charts or add new ones just like any Databricks SQL dashboard. For instance, if you want a special chart to highlight the count of zero heart rates, you could create a new query against the profile metrics table (or define a custom metric) to surface that, and pin it to the dashboard. This flexibility means the monitoring dashboard can be tailored to the metrics that matter most for your data quality goals.

## Detecting Data Quality Issues (Nulls, Zeros, Outliers)

With the monitoring set up, how do we use it to catch problems in our heart rate data? Let's walk through an example scenario of new data arriving and see how Lakehouse Monitoring helps identify issues.

Initially, our data was clean. Now, suppose a new batch of data comes in from the devices – but unfortunately, this batch has some problems:

- Some records have the name missing (NULL) because the patient info wasn't synced.
- A number of heart rate readings came in as 0. Perhaps the devices use 0 to indicate no reading or an error.
- One device had a firmware bug and logged a ridiculously high heart rate value by mistake (for example, over a million).

We append these new records to the `heart_rate_measurements` table. In SQL, that might look like:

```sql
INSERT INTO heart_rate_measurements VALUES
  -- A record with a missing name and an erroneous 0 heart rate
  (17, '52804177', NULL, '2025-10-01T12:00:00+00:00', 0),
  -- A record with a valid name but an outlier high heart rate
  (37, '65300842', 'Samuel Hughes', '2025-10-01T12:05:00+00:00', 1000052.1354);
```

*(The above are just two sample rows to illustrate; in practice, imagine ~30 new rows were ingested, with several zeros and NULL names.)*

After this data load, the table now contains these anomalies. Without monitoring, we might not notice these issues until someone queries the data or a downstream process fails. But with Lakehouse Monitoring, we can simply **trigger a refresh** of the monitor to profile the updated table (or wait for the next scheduled run).

Once the monitor refreshes, it will update the metrics:

- **Total Records** will have increased (say from 50 to 80, if 30 new rows added).
- **% Null in Name** will jump from 0% to a higher value. For example, if 10 out of the 80 entries now have name = NULL, the monitor might show "Name – 12.5% null". This immediately alerts us that name data has started to go missing.
- **Min/Max Heartrate will update**. The minimum might now be 0 (if zero values were added), and the maximum will be 1,000,052 (the outlier). These metrics turning abnormal are a red flag. The average heart rate might also skew lower due to those zeros.
- The dashboard's distribution chart for heart rates would visibly change: perhaps a spike at 0 (many zero readings) and an outlier bar way off the chart to the far right for the 1,000,052 value.
- If the monitor calculates a **percent of zeros** (which might be part of drift metrics or a custom metric), it would show a non-zero percentage now. Even if not directly shown, the zero readings are effectively counted in the distribution and in metrics like "values below X". We could configure a custom metric to explicitly count how many heart rates are exactly 0 for easier tracking.

## Best Practices for Data Quality Monitoring

The heart rate scenario illustrates general best practices in data quality monitoring:

- **Track Completeness**: Always monitor for missing or empty values in your critical fields. In Lakehouse Monitoring, the fraction of nulls per column is a key health metric. Even if a field isn't strictly required, a sudden rise in nulls often signals upstream issues. We saw this with name – monitoring let us catch that immediately.

- **Identify Invalid Placeholder Values**: Decide what constitutes an "invalid" value for your data and track it. In numeric fields, zeros or negatives might be placeholders or errors. While Lakehouse Monitoring doesn't inherently know that "0" is wrong for heart rates, you can address this by:

  - **Custom Metrics**: Define a custom metric to count how many rows have heartrate = 0. The platform allows adding custom SQL expressions as metrics during monitor setup. This way, you'd have a direct line item in your metrics for "ZeroHeartRateCount" if needed. 
  - **Expectations**: If using Delta Live Tables or certain streaming workloads, Databricks supports Data Expectations (data quality rules) to handle bad data (for example, drop records where `heart_rate = 0`). These can complement Lakehouse Monitoring by preventing bad data from ever landing in the table. In the future, Databricks plans to unify expectations with Lakehouse Monitoring for all tables.
  - Regardless, keep an eye on min/max metrics – a min of 0 in a field that shouldn't be zero is a straightforward alert.

- **Monitor Distribution and Range**: Know the expected range of your data and let the monitor tell you when you're outside of it. In our case, any heart rate above, say, 250 is likely impossible from a human. The automatic max metric caught the 1,000,052 value. Setting up an alert on the profile metrics (for example, trigger an alert if max `heart_rate > 300`) could notify the team as soon as such an anomaly is detected. With Lakehouse Monitoring, you can create SQL alerts on the underlying metric tables or directly on the dashboard charts to automate this.

- **Baseline Comparisons**: If you have a trusted baseline dataset (for example, last month's clean data or an expected distribution), use it. Lakehouse Monitoring allows specifying a baseline table for drift analysis. For instance, if we had a baseline of heart rates under normal conditions, the monitor could compare today's data against that baseline and immediately flag the large deviations in distribution or null percentages. Baselines help quantify drift in a meaningful way ("today vs. ideal dataset").

- **Regular Reviews and Updates**: Data quality metrics should be reviewed regularly by the team. Over time, you might adjust which metrics you care about. Maybe we discover that heart rates of 0 are frequent and we start filtering them out upstream – then the focus might shift to other metrics like variability of heart rates across devices. Customize your monitor as needed: add or remove metrics, change refresh schedules, or even disable monitors on tables that are no longer critical.

- **Cost and Performance**: A snapshot profile scans the whole table on each run, which could be costly for very large tables. For production, consider enabling Change Data Feed (CDF) on Delta tables and using a Time series profile if appropriate, so that monitoring only processes new data increments rather than full scans. This is more efficient at scale. In our example, if the table grows to millions of rows, switching to time-series with CDF might be wise so that each hour/day only new heart rates are processed. Snapshot is fine for smaller tables or for periodic full re-profiling.